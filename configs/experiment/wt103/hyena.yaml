# @package _global_
defaults:
  - /experiment/wt103/base.yaml

model_name: hyena-153
model:
  _name_: lm
  d_model: 864
  d_inner: 1728
  n_layer: 18
  vocab_size: 50257
  resid_dropout: 0.7
  embed_dropout: 0.7 # 0.1 from the pile, 0.2 from Github discussion
  attn_layer_idx: [] # Avoid use attention layers
  layer:
    _name_: hyena
    local_order: 3
    filter_order: 64
    num_heads: 1
    inner_factor: 1
    num_blocks: 1
    outer_mixing: False
    dropout: 0.7 # From S5

    emb_dim: 33
    l_max: ${dataset.max_length}
    # fused_fft_conv: True
    modulate: True # What's the usage of modulate

    # lr_pos_emb: 0.0 # This is the z_lr in S5, the default is 1e-5
    # modulation_lr: 0.0 # This is the deltas_lr in S5, the default is 0.0
    # lr: 0.0 # This is the implicit_filter_lr in S5, the default is
    # wd: 0.0 # This is the implicit_filter_weight_decay in S5

  # attn_cfg:
  #   use_flash_attn: False
  fused_mlp: True
  fused_dropout_add_ln: True
  residual_in_fp32: True
  pad_vocab_size_multiple: 8

optimizer:
  lr: 0.001
  weight_decay: 0.25

trainer:
  devices: 4

dataset:
  batch_size: 20 # 8 use 15G, 16 use 30G, 24 use 45G, 20 should be 37.5, reasonable
  max_length: 1024

train:
  global_batch_size: 640
