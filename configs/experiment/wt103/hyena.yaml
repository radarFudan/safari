# @package _global_
defaults:
  - /experiment/wt103/base.yaml
  - override /scheduler: cosine_warmup

model_name: hyena-153
model:
  _name_: lm
  d_model: 768
  d_inner: 3072
  n_layer: 12
  vocab_size: 50257
  resid_dropout: 0.0
  embed_dropout: 0.2 # 0.1 from the pile, 0.2 from Github discussion
  attn_layer_idx: [] # Avoid use attention layers
  layer:
    _name_: hyena
    local_order: 2
    filter_order: 128
    num_heads: 1
    inner_factor: 1
    num_blocks: 1
    outer_mixing: False
    dropout: 0.15 # From S5

    emb_dim: 5
    l_max: ${dataset.max_length}
    # fused_fft_conv: True
    modulate: True # What's the usage of modulate

    lr_pos_emb: 0.0 # This is the z_lr in S5, the default is 1e-5
    modulation_lr: 0.0 # This is the deltas_lr in S5, the default is 0.0
    lr: 0.0008 # This is the implicit_filter_lr in S5, the default is 1e-3
    wd: 0.0 # This is the implicit_filter_weight_decay in S5

  # attn_cfg:
  #   use_flash_attn: False
  fused_mlp: True
  fused_dropout_add_ln: True
  residual_in_fp32: True
  pad_vocab_size_multiple: 1

optimizer:
  lr: 0.001
  weight_decay: 0.25

trainer:
  devices: 4
  max_epochs: 100

dataset:
  batch_size: 16 # 8 use 15G, 16 use 30G, 24 use 45G, 20 should be 37.5, reasonable
  max_length: 1024

train:
  global_batch_size: 16
