# @package _global_
defaults:
  - /experiment/wt103/base.yaml
  - override /scheduler: cosine_warmup

model_name: hyena-153
model:
  _name_: lm
  d_model: 768
  d_inner: 3072
  n_layer: 12
  vocab_size: 50257
  resid_dropout: 0.0 # Not sure, from issue 28
  embed_dropout: 0.2 # 0.0 from hyena_small_150b, 0.1 from the pile
  attn_layer_idx: [] # Avoid use attention layers
  layer:
    _name_: hyena
    local_order: 2
    filter_order: 128
    num_heads: 1
    inner_factor: 1
    num_blocks: 1
    outer_mixing: False
    dropout: 0.15

    # emb_dim: 33
    # l_max: ${dataset.max_length}
    # fused_fft_conv: True
    # modulate: True # What's the usage of modulate

    lr_pos_emb: 0.0 # This is the z_lr in S5, the default is 1e-5
    modulation_lr: 0.0 # This is the deltas_lr in S5, the default is 0.0
    lr: 0.0 # This is the implicit_filter_lr in S5, the default is
    wd: 0.0 # This is the implicit_filter_weight_decay in S5

    filter_args:
      emb_dim: 5 # dim of input to MLP, augments with positional encoding
      w: 10 # frequency of periodic activations (note filter configs say 1, default in object is 10, and final hydra filter indicates 10)
      use_bias: True
      num_inner_mlps: 2
      normalized: False

  attn_cfg:
    use_flash_attn: False
  fused_mlp: True
  fused_dropout_add_ln: True
  residual_in_fp32: True
  pad_vocab_size_multiple: 1

optimizer:
  lr: 0.001
  weight_decay: 0.25

trainer:
  devices: 4

scheduler:
  num_warmup_steps: 1000
  num_training_steps: 115000
