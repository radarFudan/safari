# @package _global_
defaults:
  - /experiment/wt103/base.yaml

model_name: hyena-153
model:
  _name_: lm
  d_model: 768
  n_layer: 12
  d_inner: ${eval:4 * ${.d_model}}
  vocab_size: 50257
  resid_dropout: 0.2
  embed_dropout: 0.2 # 0.0 from hyena_small_150b, 0.1 from the pile
  attn_layer_idx: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
  # layer:
  #   _name_: hyena
  #   emb_dim: 33
  #   filter_order: 64
  #   local_order: 3
  #   l_max: 2048
  #   fused_fft_conv: False
  #   modulate: True # What's the usage of modulate
  #   w: 14
  #   lr: ${optimizer.lr}
  #   lr_pos_emb: ${optimizer.lr}
  attn_cfg:
    dropout: 0.2
  fused_mlp: True
  fused_dropout_add_ln: True
  residual_in_fp32: True
  pad_vocab_size_multiple: 8

trainer:
  accelerator: gpu
  devices: 1

dataset:
  # batch_size: 32  # Per GPU
  # batch_size: ${eval:"8 if ${train.gpu_mem} <= 16 else (16 if ${train.gpu_mem} <= 24 else 32)"}
  batch_size: 8 # 8 use 15G, 16 use 30G, 24 use 45G, 20 should be 37.5, reasonable
  max_length: 1024
