# @package _global_
defaults:
  - /experiment/wt103/base.yaml

model_name: hyena-153
model:
  _name_: lm
  d_model: 768
  d_inner: ${eval:4 * ${.d_model}}
  n_layer: 12
  vocab_size: 50257
  embed_dropout: 0.1 # 0.0 from hyena_small_150b, 0.1 from the pile
  attn_layer_idx: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
  layer:
    _name_: hyena
    emb_dim: 33
    filter_order: 64
    local_order: 3
    l_max: 2048
    fused_fft_conv: False
    modulate: True # What's the usage of modulate
    w: 14
    lr: ${optimizer.lr}
    lr_pos_emb: ${optimizer.lr}
  fused_mlp: True
  fused_dropout_add_ln: True
  residual_in_fp32: True
  pad_vocab_size_multiple: 8
