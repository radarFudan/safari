# @package _global_
defaults:
  - /experiment/wt103/base.yaml

model_name: transformer-benchmark # 153
model:
  _name_: lm_simple
  d_model: 768
  n_layer: 12
  d_inner: ${eval:4 * ${.d_model}}
  vocab_size: 50257
  resid_dropout: 0.0
  embed_dropout: 0.1
  attn_layer_idx: [1, 8]
  attn_cfg:
    causal: true
    num_heads: 12
    dropout: 0.1
  max_position_embeddings: ${dataset.__l_max} # positional embeddings
